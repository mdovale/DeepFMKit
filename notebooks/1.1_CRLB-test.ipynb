{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ba7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import DeepFMKit.core as dfm\n",
    "from scipy.special import jv\n",
    "from scipy.linalg import inv\n",
    "from scipy.stats import norm\n",
    "\n",
    "def calculate_jacobian(ndata, param):\n",
    "    \"\"\"\n",
    "    Calculates the Jacobian matrix (J) of the DFMI model.\n",
    "    (This function remains unchanged from the previous version)\n",
    "    \"\"\"\n",
    "    a, m, phi, psi = param\n",
    "    J = np.zeros((2 * ndata, 4))\n",
    "    j = np.arange(1, ndata + 1)\n",
    "    \n",
    "    phase_term = np.cos(phi + j * np.pi / 2.0)\n",
    "    cos_jpsi = np.cos(j * psi)\n",
    "    sin_jpsi = np.sin(j * psi)\n",
    "    \n",
    "    bessel_j = jv(j, m)\n",
    "    bessel_deriv = 0.5 * (jv(j - 1, m) - jv(j + 1, m))\n",
    "    \n",
    "    common_term = a * phase_term * bessel_j\n",
    "    model_q = common_term * cos_jpsi\n",
    "    model_i = -common_term * sin_jpsi\n",
    "    \n",
    "    if a != 0:\n",
    "        J[:ndata, 0] = model_q / a\n",
    "        J[ndata:, 0] = model_i / a\n",
    "\n",
    "    common_deriv_term_m = a * phase_term * bessel_deriv\n",
    "    J[:ndata, 1] = common_deriv_term_m * cos_jpsi\n",
    "    J[ndata:, 1] = -common_deriv_term_m * sin_jpsi\n",
    "\n",
    "    phase_deriv_term = np.cos(phi + j * np.pi / 2.0 + np.pi / 2.0)\n",
    "    common_deriv_term_phi = a * phase_deriv_term * bessel_j\n",
    "    J[:ndata, 2] = common_deriv_term_phi * cos_jpsi\n",
    "    J[ndata:, 2] = -common_deriv_term_phi * sin_jpsi\n",
    "\n",
    "    J[:ndata, 3] = common_term * -sin_jpsi * j\n",
    "    J[ndata:, 3] = -common_term * cos_jpsi * j\n",
    "    \n",
    "    return J\n",
    "\n",
    "# In analyze_precision.py\n",
    "\n",
    "def calculate_m_precision(m_range, ndata, snr_db, buffer_size):\n",
    "    \"\"\"\n",
    "    Core calculation function for statistical uncertainty of 'm'.\n",
    "    \n",
    "    This version correctly accounts for the averaging gain from using\n",
    "    a buffer of a given size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m_range : array_like\n",
    "        The range of modulation depths 'm' to analyze.\n",
    "    ndata : int\n",
    "        Number of harmonics to use in the fit.\n",
    "    snr_db : float\n",
    "        Signal-to-Noise Ratio in dB, defined for the time-domain signal.\n",
    "    buffer_size : int\n",
    "        The number of samples (R) in the buffer being averaged. This\n",
    "        is the source of the processing gain.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        An array of the statistical uncertainty (delta_m) for each m in m_range.\n",
    "    \"\"\"\n",
    "    param_fixed = np.array([1.0, 0, np.pi/4, 0.0]) # a, m, phi, psi\n",
    "    \n",
    "    # SNR is a power ratio, so we use 10*log10.\n",
    "    snr_linear_power = 10**(snr_db / 10.0)\n",
    "    \n",
    "    # Assume signal amplitude a=1. Signal power is a^2/2 = 0.5.\n",
    "    signal_power = 0.5\n",
    "    noise_power_time_domain = signal_power / snr_linear_power\n",
    "    \n",
    "    # The variance on the I/Q measurement is the time-domain noise variance\n",
    "    # divided by the buffer size (averaging gain). A factor of 2 is also\n",
    "    # included for the properties of quadrature demodulation.\n",
    "    noise_variance_iq = noise_power_time_domain / (2 * buffer_size)\n",
    "    \n",
    "    delta_m_list = []\n",
    "    for m_true in m_range:\n",
    "        param_fixed[1] = m_true\n",
    "        J = calculate_jacobian(ndata, param_fixed)\n",
    "        JTJ = J.T @ J\n",
    "        try:\n",
    "            # The CRLB for the parameters\n",
    "            covariance_matrix = noise_variance_iq * inv(JTJ)\n",
    "            delta_m = np.sqrt(covariance_matrix[1, 1])\n",
    "            delta_m_list.append(delta_m)\n",
    "        except np.linalg.LinAlgError:\n",
    "            delta_m_list.append(np.inf)\n",
    "            \n",
    "    return np.array(delta_m_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed55278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_fitter_efficiency(m_true=15.5, ndata=15, snr_db=70.0, n_trials=500):\n",
    "    \"\"\"\n",
    "    Performs a Monte Carlo simulation to test the NLS fitter's performance\n",
    "    against the theoretical Cramér-Rao Lower Bound (CRLB).\n",
    "\n",
    "    This function verifies that the fitter is statistically \"efficient\" by\n",
    "    comparing its measured precision from repeated noisy simulations against\n",
    "    the theoretical best-case precision predicted by the CRLB. The CRLB\n",
    "    calculation is corrected to account for the processing gain from\n",
    "    averaging over the sample buffer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m_true : float, optional\n",
    "        The ground-truth modulation depth to test.\n",
    "    ndata : int, optional\n",
    "        The number of harmonics to use in the fit.\n",
    "    snr_db : float, optional\n",
    "        The target Signal-to-Noise Ratio (in dB) for the simulation. The SNR\n",
    "        is defined as the ratio of the AC signal power to the time-domain\n",
    "        white noise power.\n",
    "    n_trials : int, optional\n",
    "        The number of Monte Carlo trials to run. A higher number gives a\n",
    "        more reliable estimate of the measured precision.\n",
    "    \"\"\"\n",
    "    # --- 1. Setup & Theoretical Calculation ---\n",
    "    print(\"=\"*60)\n",
    "    print(\"Fitter Efficiency Validation: Comparing Measured vs. Theoretical Precision\")\n",
    "    print(f\"Parameters: m = {m_true}, ndata = {ndata}, SNR = {snr_db} dB\")\n",
    "    print(f\"Number of Monte Carlo trials: {n_trials}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Define all simulation parameters in one place for clarity\n",
    "    sim_params = {\n",
    "        'm': m_true,\n",
    "        'amp': 1.0,\n",
    "        'f_mod': 1000,\n",
    "        'f_samp': 200000,\n",
    "        'fit_n': 20\n",
    "    }\n",
    "\n",
    "    # Calculate the derived buffer size (R) and duration\n",
    "    buffer_size = int(sim_params['fit_n'] * (sim_params['f_samp'] / sim_params['f_mod']))\n",
    "    n_seconds_per_buffer = sim_params['fit_n'] / sim_params['f_mod']\n",
    "\n",
    "    # --- 2. Calculate the Theoretical CRLB ---\n",
    "    # This call now includes the buffer_size, which is the crucial correction.\n",
    "    print(\"\\nCalculating theoretical precision (CRLB)...\")\n",
    "    delta_m_crlb = calculate_m_precision(np.array([m_true]), ndata, snr_db, buffer_size)[0]\n",
    "    print(f\"Theoretical Precision (CRLB): δm = {delta_m_crlb:.4e}\")\n",
    "\n",
    "    # --- 3. Initialize Framework and Run Monte Carlo Simulation ---\n",
    "    dff = dfm.DeepFitFramework()\n",
    "    label = \"efficiency_test\"\n",
    "    dff.new_sim(label)\n",
    "    \n",
    "    # Configure the simulation object from our parameter dictionary\n",
    "    sim_config = dff.sims[label]\n",
    "    for key, value in sim_params.items():\n",
    "        setattr(sim_config, key, value)\n",
    "    \n",
    "    m_estimates = []\n",
    "    # The initial guess for the fitter should be slightly off to ensure it does work.\n",
    "    initial_guess = np.array([sim_config.amp, m_true * 0.95, 0.1, 0.1])\n",
    "    \n",
    "    print(f\"\\nRunning {n_trials} Monte Carlo simulations...\")\n",
    "    for i in tqdm(range(n_trials)):\n",
    "        # Use the simple SNR-based simulation method\n",
    "        dff.simulate(\n",
    "            label,\n",
    "            n_seconds=n_seconds_per_buffer,\n",
    "            snr_db=snr_db,\n",
    "            trial_num=i  # Ensures each noise realization is unique\n",
    "        )\n",
    "        \n",
    "        # Run the fit on the newly generated data buffer\n",
    "        fit_result = dff._fit_single_buffer(label, 0, buffer_size, ndata, initial_guess)\n",
    "        m_estimates.append(fit_result['m'])\n",
    "\n",
    "    # --- 4. Analyze and Print the Results ---\n",
    "    m_estimates = np.array(m_estimates)\n",
    "    delta_m_measured = np.std(m_estimates)\n",
    "    mean_m_measured = np.mean(m_estimates)\n",
    "    bias = mean_m_measured - m_true\n",
    "    \n",
    "    # Estimator efficiency is the ratio of theoretical best variance to actual variance\n",
    "    efficiency = (delta_m_crlb**2 / delta_m_measured**2) * 100 if delta_m_measured > 0 else 0\n",
    "\n",
    "    print(\"\\n--- Results ---\")\n",
    "    print(f\"Measured Mean of estimates:  <m> = {mean_m_measured:.6f}\")\n",
    "    print(f\"Estimator Bias (<m> - m_true):   = {bias:.4e}\")\n",
    "    print(f\"Measured Precision (Std Dev): δm = {delta_m_measured:.4e}\")\n",
    "    print(f\"Estimator Efficiency (CRLB² / Measured²): {efficiency:.1f}%\")\n",
    "\n",
    "    # --- 5. Plot the Distribution of Estimates for Visual Confirmation ---\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Plot the histogram of results\n",
    "    count, bins, _ = ax.hist(m_estimates, bins=50, density=True, \n",
    "                             label=f'Histogram of $\\hat{{m}}$ estimates\\n($N_{{trials}}={n_trials}$)', \n",
    "                             alpha=0.6, color='tab:blue')\n",
    "\n",
    "    # Overlay a fitted Gaussian curve\n",
    "    mu, std = norm.fit(m_estimates)\n",
    "    p = norm.pdf(bins, mu, std)\n",
    "    ax.plot(bins, p, 'k--', linewidth=2, label='Fitted Normal Distribution')\n",
    "\n",
    "    # Add vertical lines for key values\n",
    "    ax.axvline(m_true, color='red', linestyle='-', linewidth=2, label=f'True m = {m_true:.4f}')\n",
    "    ax.axvline(mean_m_measured, color='black', linestyle='--', linewidth=2, label=f'Measured Mean = {mean_m_measured:.4f}')\n",
    "\n",
    "    title_text = f\"Estimator Performance vs. CRLB (m={m_true}, ndata={ndata}, SNR={snr_db}dB)\"\n",
    "    ax.set_title(title_text, fontsize=16)\n",
    "    ax.set_xlabel('Estimated Modulation Depth ($\\hat{m}$)', fontsize=14)\n",
    "    ax.set_ylabel('Probability Density', fontsize=14)\n",
    "    \n",
    "    # Add a text box with the key results for easy reading on the plot\n",
    "    results_str = (f\"Bias = {bias:.2e}\\n\"\n",
    "                   f\"$\\delta m_{{measured}}$ = {delta_m_measured:.3e}\\n\"\n",
    "                   f\"$\\delta m_{{CRLB}}$ = {delta_m_crlb:.3e}\\n\"\n",
    "                   f\"Efficiency = {efficiency:.1f}%\")\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax.text(0.05, 0.95, results_str, transform=ax.transAxes, fontsize=12,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle=':')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "validate_fitter_efficiency(m_true=4.8, ndata=10, snr_db=0.0, n_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69843ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bias_variance_tradeoff(m_range=np.linspace(2.0, 10.0, 81), \n",
    "                                   ndata=10, \n",
    "                                   snr_db=70.0, \n",
    "                                   n_trials=500):\n",
    "    \"\"\"\n",
    "    Performs a Monte Carlo simulation across a range of modulation depths (m)\n",
    "    to analyze the estimator's bias-variance trade-off.\n",
    "\n",
    "    This function measures both the bias and the variance of the NLS fitter\n",
    "    and compares its variance to the Cramér-Rao Lower Bound (CRLB) to\n",
    "    calculate its statistical efficiency. The results reveal regions where the\n",
    "    estimator becomes biased to achieve a lower variance, resulting in an\n",
    "    apparent efficiency greater than 100%.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m_range : array_like, optional\n",
    "        The range of ground-truth modulation depths 'm' to test.\n",
    "    ndata : int, optional\n",
    "        The number of harmonics to use in the fit.\n",
    "    snr_db : float, optional\n",
    "        The target Signal-to-Noise Ratio (in dB) for the simulations.\n",
    "    n_trials : int, optional\n",
    "        The number of Monte Carlo trials to run for each value of 'm'.\n",
    "    \"\"\"\n",
    "    # --- 1. Setup ---\n",
    "    print(\"=\"*60)\n",
    "    print(\"Analyzing Estimator Bias-Variance Trade-off\")\n",
    "    print(f\"m range: {m_range.min()} to {m_range.max()} ({len(m_range)} points)\")\n",
    "    print(f\"ndata={ndata}, SNR={snr_db} dB, n_trials={n_trials}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Simulation parameters\n",
    "    sim_params = { 'amp': 1.0, 'f_mod': 1000, 'f_samp': 200000, 'fit_n': 20 }\n",
    "    buffer_size = int(sim_params['fit_n'] * (sim_params['f_samp'] / sim_params['f_mod']))\n",
    "    n_seconds_per_buffer = sim_params['fit_n'] / sim_params['f_mod']\n",
    "\n",
    "    # --- 2. Initialize Framework and Data Storage ---\n",
    "    dff = dfm.DeepFitFramework()\n",
    "    label = \"tradeoff_test\"\n",
    "    dff.new_sim(label)\n",
    "    sim_config = dff.sims[label]\n",
    "    for key, value in sim_params.items():\n",
    "        setattr(sim_config, key, value)\n",
    "    \n",
    "    bias_list = []\n",
    "    efficiency_list = []\n",
    "    \n",
    "    # --- 3. Loop over the m_range and run Monte Carlo for each point ---\n",
    "    print(\"\\nRunning Monte Carlo analysis for each m value...\")\n",
    "    for m_true in tqdm(m_range):\n",
    "        sim_config.m = m_true\n",
    "        m_estimates = np.zeros(n_trials)\n",
    "        initial_guess = np.array([sim_config.amp, m_true * 0.95, 0.1, 0.1])\n",
    "\n",
    "        for i in range(n_trials):\n",
    "            dff.simulate(label, n_seconds=n_seconds_per_buffer, snr_db=snr_db, trial_num=i)\n",
    "            fit_result = dff._fit_single_buffer(label, 0, buffer_size, ndata, initial_guess)\n",
    "            m_estimates[i] = fit_result['m']\n",
    "\n",
    "        # --- 4. Calculate Bias, Variance, and Efficiency for this m_true ---\n",
    "        delta_m_crlb = calculate_m_precision(np.array([m_true]), ndata, snr_db, buffer_size)[0]\n",
    "        \n",
    "        delta_m_measured = np.std(m_estimates)\n",
    "        mean_m_measured = np.mean(m_estimates)\n",
    "        \n",
    "        bias = mean_m_measured - m_true\n",
    "        \n",
    "        # Avoid division by zero if a fit completely fails\n",
    "        if delta_m_measured > 0:\n",
    "            efficiency = (delta_m_crlb**2 / delta_m_measured**2) * 100\n",
    "        else:\n",
    "            efficiency = 0.0\n",
    "\n",
    "        bias_list.append(bias)\n",
    "        efficiency_list.append(efficiency)\n",
    "\n",
    "    # --- 5. Plot the Final Results ---\n",
    "    bias_array = np.array(bias_list)\n",
    "    efficiency_array = np.array(efficiency_list)\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Plot 1: Efficiency on the left y-axis\n",
    "    color1 = 'tab:blue'\n",
    "    ax1.set_xlabel('True Modulation Depth (m)', fontsize=14)\n",
    "    ax1.set_ylabel('Estimator Efficiency (%)', color=color1, fontsize=14)\n",
    "    ax1.plot(m_range, efficiency_array, color=color1, linewidth=2, label='Measured Efficiency')\n",
    "    ax1.axhline(100, color=color1, linestyle='--', label='100% Efficiency (CRLB)')\n",
    "    ax1.tick_params(axis='y', labelcolor=color1)\n",
    "    ax1.grid(True, which='both', linestyle=':')\n",
    "    \n",
    "    # Plot 2: Bias on the right y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    color2 = 'tab:red'\n",
    "    ax2.set_ylabel('Estimator Bias (m_measured - m_true)', color=color2, fontsize=14)\n",
    "    # Normalize the bias by the CRLB to make it unitless and comparable\n",
    "    # A value of 1 means the bias is as large as the expected statistical error.\n",
    "    normalized_bias = bias_array / calculate_m_precision(m_range, ndata, snr_db, buffer_size)\n",
    "    ax2.plot(m_range, normalized_bias, color=color2, linestyle='-', marker='.', markersize=4, alpha=0.7, label='Normalized Bias ($\\sigma$ units)')\n",
    "    ax2.axhline(0, color=color2, linestyle='--')\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "    \n",
    "    # Final plot aesthetics\n",
    "    ax1.set_title('Estimator Efficiency and Bias vs. Modulation Depth', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Combine legends from both axes\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines + lines2, labels + labels2, loc='upper right')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Run the analysis with parameters that highlight the issue\n",
    "# We choose a range around m=4.8 where the effect was seen\n",
    "m_test_range = np.linspace(3.0, 20.0, 100) # High resolution in the interesting region\n",
    "analyze_bias_variance_tradeoff(m_range=m_test_range, ndata=10, snr_db=70.0, n_trials=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23bc41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
